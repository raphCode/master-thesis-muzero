\chapter{Theory}
% Ich finde ein extra Theorie Kapitel gut, in dem man die theoretischen Grundlagend
% zusammenfasst, die nötig sind um die verwendeten Algorithmen zu verstehen. 
% In Research-papern ist hierfür meist kein Platz aber in einer Abschlussarbeit kann man
% damit gut zeigen, dass man das Material verstanden hat.

\section{Principles}
\question{unsure about the title}

\question{should I add more maths and formulas? They don't help my understanding, but
everyone uses them...}

Reinforcement learning is about finding an optimal behaviour in a given enviroment by
trail-and-error interaction.
These environments can be decomposed into a number of steps, for example a game made up of
single moves.
At each step, the agent to be trained perceives the current state of the environment and
decides on an action to perform.
This yields an immediate scalar reward value judging the chosen action's quality and leads
to a new state.
The behaviour to learn is a mapping from states to actions, also termed policy.
Its performance is evaluated by calculating the cumultative reward throughout the episode,
that is all steps until the end of e.g. the game.
\question{emphasize keywords (action, state, reward, ...) the first time they are
mentioned?}
\todo{image?}
\todo{cite}

The learning takes place by repeatedly stepping through the environment, evaluating the
current policy for each state to choose the next action.
The only signal to learn from is the reward returned at each step, which decides how good
the taken action was.
Actions that lead to high rewards are reinforced, while actions that yield negative
rewards are discouraged.
Often the reward signal is said to be sparse, meaning most of the time it is actually
zero, which provides no learning feedback.
Many games fall in this category, since only at the very end it is known which player has
won.
Thus, an additional challenge is then to assign the reward to the deciding actions, that
is answering the question:
Which moves attributed how much to the final result of the game?
\todo{cite}

\begin{itemize}
    \item Evolution:
    \begin{itemize}
        \item AlphaGo
        \item AlphaGo Zero \cite{alphago_zero}
        \question[noinline]{I probably need to adjust the citation styles, they do
        not match your template?}
        \item AlphaZero \cite{alphazero}
        \item MuZero \cite{muzero} $\Longleftarrow$ we are here
        \item Current State of the art: Player of Games
        \item ? Sampled MuZero
        \item ? MuZero Unplugged
    \end{itemize}
    \item Original Paper has special requirements for the type of games
    that can be learned:
    \todo{Check if the \cite{muzero} mentions these restrictions, I actually found
    these in \url{https://sebastianbodenstein.net/post/alphazero/}}
    \question{please comment on the general situation: "found information / idea on
    the internet, should I mention the link or just confirm the information exists in
    the original paper and only reference that?}
    \begin{itemize}
        \item sequential (informaticup game is not)
        \item single player or two player
        \item two player: zero sum (optimisations in the search tree:
        flipping rewards every other turn)
        \item transitive "better than" relation on game strategies: unique best
        strategy must exist (counterexample: starcraft)
    \end{itemize}
    \item Basic techniques:
    \begin{itemize}
        \item Monte Carlo Tree Search \cite{mcts_survey}
        \item Model-based RL
    \end{itemize}
\end{itemize}
