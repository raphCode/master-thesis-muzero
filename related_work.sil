\begin{sile}

\chapter{Related Work}
% Hier beschreibst du welche verwandte Arbeiten es gibt und wie deine Arbeit sich davon
% abhebt.
% Nach Prof. Andr√© ist ein related work perfekt, wenn der Leser keines der referenzierten
% paper selbst lesen muss um die Ideen darin zu verstehen.

While no multiplayer version for MuZero itself exists, \citeauthor{multiplayer_alphazero}
extended the predecessor algorithm AlphaZero to multiplayer
capabilities\cite{multiplayer_alphazero}.

Importantly, they relax the assumption of the game being zero-sum:
A zero-sum game involves two players where player one's gain comes with an equivalent loss
for the other player.
The AlphaZero (and in turn MuZero) algorithm make use of this fact by directly predicting
how good the current position is for the current player by a scalar \val output of the
network\cite{alphazero}\cite{muzero}.
The multiplayer extension predicts a \term{value vector} instead, which provides an
estimate of the expected utility of the state for each player independently.

Likewise, they also extend the game to return scores for each player at the end instead of
a single outcome.
Naturally, the algorithm rotates over the list of all players instead of alternating
between two players.\cite{alphazero}
They evaluate their work on multiplayer versions of Connect 4 and Tic-Tac-Toe:
The networks learn to encode knowledge of the game into search, indicating that the
proposed multiplayer strategy works in principle.
Performance-wise the algorithm places itself below human experts.

\term{EfficientZero} by \citeauthor{efficientzero} is a modification of MuZero to achieve
similar performance with less training data.
To achieve this increase in \term{sample efficiency} they propose three changes:

First, they notice that the dynamics model should map to same \latrep as
the \rnet for the same game states.
By adding a similarity term to the loss function, these network outputs are encouraged to
converge.
This provides a richer training signal since the \latrep is usually a very
wide tensor.\cite{efficientzero}
Compare this to the default MuZero implementation, where the only training signals stem
from the scalars \rew and \val\cite{muzero}.

% TODO: probably scope these command definitions?
\define[command=St]{\math{S_t}}
\define[command=St1]{\math{S_{t+1}}}
\define[command=Sht1]{\math{S\hat_{t+1}}} % TODO: formula broken

\todo{Add image}
The current and next game \term{observation} \math{O_t} and \math{O_{t+1}} is fed through
the \rnet, yielding the \latrep \St respectively \St1.
From \St and the corresponding action \math{a_t} the \dnet predicts the \latrep \Sht1.
The similarity loss is now introduced between \St1 and \Sht1, termed as
\term{Self-Supervised Consistency Loss}.\cite{efficientzero}

A stop-gradient operation is employed in the path of \St1, meaning that the training
information from the similarity loss are not applied to the \rnet prediction \St1.
The authors justify this decision by treating \St1 as the more accurate representation and
therefore using it as a target for the \dnet.\cite{efficientzero}
In this work, I draw inspiration from this idea, but remove the asymmetry in training
caused by the stop-gradient operation.\todo{add forward reference to relevant section}

The second change aims to improve the selection of nodes during the \mcts{}:
MuZero selects according to the \term{UCT} formula\todo{add reference}, which involves
summing the predicted \rews\footnote{actually, the \rews are discounted according to the
\term{discount factor} \math{\gamma}} for each step in the search tree.
The authors claim that in most cases it is not important at which exact timestep a certain
\rew occurs, only that it eventually occurs.
They argue for this assumption from the way humans reason about games, and connect it to
the \term{state aliasing} problem.\cite{efficientzero}
For a more detailed discussion see\todo{add reference}.

Their suggestion is to introduce a new neural network to predict the sum of \rews for a
given sequence of states in an end-to-end method.
This effectively sidesteps the question of \em{when} a certain \rew occurs.
In the paper, this is called \term{End-To-End Prediction of the Value Prefix} and
implemented with a LSTM architecture.\cite{efficientzero}

Third, the authors mitigate \term{off-policy} issues when reusing older game trajectories
for training, which is crucial to achieve high \term{sample efficiency}.
The original MuZero paper already presented the \term{Reanalyze} variant which re-runs the
MCTS on old trajectories with the newest network parameters to provide more accurate
predictions of the \pol\cite{muzero}.
The \vtarget is computed by summing future \rews over the \term{n-step} horizon, which are
fixed due to the recorded trajectory, so it naturally suffers from \term{off-policy}
issues\cite{muzero}.
\citeauthor{efficientzero} propose to only use \rews over a smaller horizon, with the size
decreasing for older trajectories to reduce the off-policy divergence.
In addition, the \vtarget is \term[key=bootstrapping]{bootstrapped} using a \val
prediction for the last state in the horizon, which is obtained in MuZero \term{Reanalyze}
using a raw prediction from the \pnet\cite{muzero}.
EfficientZero instead runs a full MCTS at the last horizon state, and uses the \val
estimate from the root node for \term{bootstrapping}.
This estimate is an average of multiple predictions throughout the search tree and is
therefore considered a more accurate value.\cite{efficientzero}

\end{sile}
