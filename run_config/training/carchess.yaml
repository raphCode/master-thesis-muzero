batch_size: 128
train_selfplay_ratio: 50
unroll_length: 7
discount_factor: 1
n_step_horizon: ${.max_steps_per_game}
replay_buffer_size: 50_000
max_steps_per_game: 1000
latent_dist_pnorm: 2
optimizer:
    _target_: torch.optim.Adam
    weight_decay: 1e-3
learning_rates:
    base: 2e-4
    dynamics: 1
    prediction: 1
    representation: 1
loss_weights:
    latent: 0.1
    value: 0.2
    reward: 1
    policy: 1e-2
    turn: 1
