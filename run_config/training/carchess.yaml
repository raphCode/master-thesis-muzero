batch_size: 512
train_selfplay_ratio: 100
unroll_length: 6
random_unroll_length: false
discount_factor: 1
n_step_horizon: ${.max_steps_per_game}
replay_buffer_size: 30_000
random_play_steps: 100_000
max_steps_per_game: 1000
max_total_steps: -1
absorbing_terminal_states: false
latent_loss_detach: false
grad_norm_clip: 8e-3
optimizer:
    _target_: madgrad.MADGRAD
learning_rates:
    base: 1e-3
    dynamics: 1
    prediction: 1
    representation: 1
loss_weights:
    latent: 0.1
    value: 0.1
    reward: 1
    policy: 0.1
    turn: 1
