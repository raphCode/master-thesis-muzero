batch_size: 512
train_selfplay_ratio: 400
unroll_length: 3
discount_factor: 0.98
n_step_horizon: ${.max_steps_per_game}
replay_buffer_size: 20_000
random_play_steps: 10_000
max_steps_per_game: 1_000
max_total_steps: 100_000
absorbing_terminal_states: false
latent_loss_detach: false
optimizer:
    _target_: torch.optim.Adam
    weight_decay: 1e-5
learning_rates:
    base: 3e-4
    dynamics: 1
    prediction: 1
    representation: 1
loss_weights:
    latent: 0.5
    value: 0.1
    reward: 1
    policy: 0.01
    turn: 1
